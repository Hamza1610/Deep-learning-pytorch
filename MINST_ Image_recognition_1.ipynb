{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image recognition using MINST dataset with class model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitsRecogniserModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 7, kernel_size=3, stride=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(7, 16, kernel_size=2, stride=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(-1, 16 * 6 * 6)  # Adjust to match flattened size\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# Test the model dimensions\n",
    "test_input = torch.randn(1, 1, 28, 28)  # Batch size 1, 1 channel, 28x28 image\n",
    "model = DigitsRecogniserModel()\n",
    "output = model(test_input)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor() # Convert the datatset to 4-dimenentional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = datasets.MNIST(root='/digits_data', train=True, download=True, transform=transform) \n",
    "testing_dataset = datasets.MNIST(root='/digits_data', train=False, download=True, transform=transform) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=training_dataset, batch_size=5, shuffle=True)\n",
    "test_loader = DataLoader(dataset=training_dataset, batch_size=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset MNIST\n",
       "     Number of datapoints: 60000\n",
       "     Root location: /digits_data\n",
       "     Split: Train\n",
       "     StandardTransform\n",
       " Transform: ToTensor(),\n",
       " Dataset MNIST\n",
       "     Number of datapoints: 10000\n",
       "     Root location: /digits_data\n",
       "     Split: Test\n",
       "     StandardTransform\n",
       " Transform: ToTensor())"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset, testing_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DigitsRecogniserModel(\n",
       "  (conv1): Conv2d(1, 7, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(7, 16, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DigitsRecogniserModel()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare loss function and optimiser to calculate loss and optimize data\n",
    "loss_function = nn.CrossEntropyLoss() # add a loss function\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.01) # add optimser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# tEST FOR DIMENTIONS Mathc for model\n",
    "test_input = torch.randn(1, 1, 28, 28)  # Batch size = 1, channels = 1, image size = 28x28\n",
    "output = model(test_input)\n",
    "print(output.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch : 0\t Batch : 500\t Loss : 0.867453932762146\n",
      " Epoch : 0\t Batch : 1000\t Loss : 1.0533723831176758\n",
      " Epoch : 0\t Batch : 1500\t Loss : 0.010403141379356384\n",
      " Epoch : 0\t Batch : 2000\t Loss : 0.011599701829254627\n",
      " Epoch : 0\t Batch : 2500\t Loss : 0.1807120144367218\n",
      " Epoch : 0\t Batch : 3000\t Loss : 0.21489953994750977\n",
      " Epoch : 0\t Batch : 3500\t Loss : 0.43504539132118225\n",
      " Epoch : 0\t Batch : 4000\t Loss : 1.0761748552322388\n",
      " Epoch : 0\t Batch : 4500\t Loss : 1.0951368808746338\n",
      " Epoch : 0\t Batch : 5000\t Loss : 0.9545404314994812\n",
      " Epoch : 0\t Batch : 5500\t Loss : 0.0018839600961655378\n",
      " Epoch : 0\t Batch : 6000\t Loss : 7.986945092852693e-06\n",
      " Epoch : 0\t Batch : 6500\t Loss : 0.16693586111068726\n",
      " Epoch : 0\t Batch : 7000\t Loss : 0.011940227821469307\n",
      " Epoch : 0\t Batch : 7500\t Loss : 1.839482069015503\n",
      " Epoch : 0\t Batch : 8000\t Loss : 1.4149731397628784\n",
      " Epoch : 0\t Batch : 8500\t Loss : 0.10825206339359283\n",
      " Epoch : 0\t Batch : 9000\t Loss : 1.5358644723892212\n",
      " Epoch : 0\t Batch : 9500\t Loss : 0.026699591428041458\n",
      " Epoch : 0\t Batch : 10000\t Loss : 0.4838765561580658\n",
      " Epoch : 0\t Batch : 10500\t Loss : 0.10971985012292862\n",
      " Epoch : 0\t Batch : 11000\t Loss : 0.014889152720570564\n",
      " Epoch : 0\t Batch : 11500\t Loss : 0.14312270283699036\n",
      " Epoch : 0\t Batch : 12000\t Loss : 0.4702538549900055\n",
      " Epoch : 1\t Batch : 500\t Loss : 0.12854406237602234\n",
      " Epoch : 1\t Batch : 1000\t Loss : 1.4607579708099365\n",
      " Epoch : 1\t Batch : 1500\t Loss : 3.723751433426514e-05\n",
      " Epoch : 1\t Batch : 2000\t Loss : 0.5341063737869263\n",
      " Epoch : 1\t Batch : 2500\t Loss : 0.02428579330444336\n",
      " Epoch : 1\t Batch : 3000\t Loss : 1.501874566078186\n",
      " Epoch : 1\t Batch : 3500\t Loss : 0.5437515377998352\n",
      " Epoch : 1\t Batch : 4000\t Loss : 0.7293663620948792\n",
      " Epoch : 1\t Batch : 4500\t Loss : 0.4227493703365326\n",
      " Epoch : 1\t Batch : 5000\t Loss : 0.5151233077049255\n",
      " Epoch : 1\t Batch : 5500\t Loss : 0.5648229718208313\n",
      " Epoch : 1\t Batch : 6000\t Loss : 0.31098848581314087\n",
      " Epoch : 1\t Batch : 6500\t Loss : 0.9869985580444336\n",
      " Epoch : 1\t Batch : 7000\t Loss : 2.2669029235839844\n",
      " Epoch : 1\t Batch : 7500\t Loss : 2.354321241378784\n",
      " Epoch : 1\t Batch : 8000\t Loss : 2.3261959552764893\n",
      " Epoch : 1\t Batch : 8500\t Loss : 2.1597204208374023\n",
      " Epoch : 1\t Batch : 9000\t Loss : 2.2139742374420166\n",
      " Epoch : 1\t Batch : 9500\t Loss : 1.6886218786239624\n",
      " Epoch : 1\t Batch : 10000\t Loss : 2.1085548400878906\n",
      " Epoch : 1\t Batch : 10500\t Loss : 1.8092283010482788\n",
      " Epoch : 1\t Batch : 11000\t Loss : 1.8790028095245361\n",
      " Epoch : 1\t Batch : 11500\t Loss : 1.8792622089385986\n",
      " Epoch : 1\t Batch : 12000\t Loss : 0.8397197723388672\n",
      " Epoch : 2\t Batch : 500\t Loss : 1.726641058921814\n",
      " Epoch : 2\t Batch : 1000\t Loss : 2.247993230819702\n",
      " Epoch : 2\t Batch : 1500\t Loss : 2.3297040462493896\n",
      " Epoch : 2\t Batch : 2000\t Loss : 2.3035902976989746\n",
      " Epoch : 2\t Batch : 2500\t Loss : 2.213541030883789\n",
      " Epoch : 2\t Batch : 3000\t Loss : 2.272926092147827\n",
      " Epoch : 2\t Batch : 3500\t Loss : 2.3112165927886963\n",
      " Epoch : 2\t Batch : 4000\t Loss : 2.3110756874084473\n",
      " Epoch : 2\t Batch : 4500\t Loss : 2.3026061058044434\n",
      " Epoch : 2\t Batch : 5000\t Loss : 2.343924045562744\n",
      " Epoch : 2\t Batch : 5500\t Loss : 2.3308708667755127\n",
      " Epoch : 2\t Batch : 6000\t Loss : 2.2893054485321045\n",
      " Epoch : 2\t Batch : 6500\t Loss : 2.2993407249450684\n",
      " Epoch : 2\t Batch : 7000\t Loss : 2.3596744537353516\n",
      " Epoch : 2\t Batch : 7500\t Loss : 2.3114593029022217\n",
      " Epoch : 2\t Batch : 8000\t Loss : 2.3597843647003174\n",
      " Epoch : 2\t Batch : 8500\t Loss : 2.3574700355529785\n",
      " Epoch : 2\t Batch : 9000\t Loss : 2.3521463871002197\n",
      " Epoch : 2\t Batch : 9500\t Loss : 2.2840871810913086\n",
      " Epoch : 2\t Batch : 10000\t Loss : 2.231736421585083\n",
      " Epoch : 2\t Batch : 10500\t Loss : 2.318530321121216\n",
      " Epoch : 2\t Batch : 11000\t Loss : 2.2787888050079346\n",
      " Epoch : 2\t Batch : 11500\t Loss : 2.297597646713257\n",
      " Epoch : 2\t Batch : 12000\t Loss : 2.384981632232666\n",
      " Epoch : 3\t Batch : 500\t Loss : 2.285822629928589\n",
      " Epoch : 3\t Batch : 1000\t Loss : 2.3323428630828857\n",
      " Epoch : 3\t Batch : 1500\t Loss : 2.3095948696136475\n",
      " Epoch : 3\t Batch : 2000\t Loss : 2.3070101737976074\n",
      " Epoch : 3\t Batch : 2500\t Loss : 2.273890972137451\n",
      " Epoch : 3\t Batch : 3000\t Loss : 2.258833885192871\n",
      " Epoch : 3\t Batch : 3500\t Loss : 2.3013224601745605\n",
      " Epoch : 3\t Batch : 4000\t Loss : 2.298448085784912\n",
      " Epoch : 3\t Batch : 4500\t Loss : 2.319016933441162\n",
      " Epoch : 3\t Batch : 5000\t Loss : 2.279758930206299\n",
      " Epoch : 3\t Batch : 5500\t Loss : 2.3743739128112793\n",
      " Epoch : 3\t Batch : 6000\t Loss : 2.307678699493408\n",
      " Epoch : 3\t Batch : 6500\t Loss : 2.3013949394226074\n",
      " Epoch : 3\t Batch : 7000\t Loss : 2.3530898094177246\n",
      " Epoch : 3\t Batch : 7500\t Loss : 2.2919704914093018\n",
      " Epoch : 3\t Batch : 8000\t Loss : 2.3192813396453857\n",
      " Epoch : 3\t Batch : 8500\t Loss : 2.268557548522949\n",
      " Epoch : 3\t Batch : 9000\t Loss : 2.360090732574463\n",
      " Epoch : 3\t Batch : 9500\t Loss : 2.351499319076538\n",
      " Epoch : 3\t Batch : 10000\t Loss : 2.284740447998047\n",
      " Epoch : 3\t Batch : 10500\t Loss : 2.296522617340088\n",
      " Epoch : 3\t Batch : 11000\t Loss : 2.3445258140563965\n",
      " Epoch : 3\t Batch : 11500\t Loss : 2.2910029888153076\n",
      " Epoch : 3\t Batch : 12000\t Loss : 2.256690502166748\n",
      " Epoch : 4\t Batch : 500\t Loss : 2.323106050491333\n",
      " Epoch : 4\t Batch : 1000\t Loss : 2.3385441303253174\n",
      " Epoch : 4\t Batch : 1500\t Loss : 2.3360354900360107\n",
      " Epoch : 4\t Batch : 2000\t Loss : 2.299107551574707\n",
      " Epoch : 4\t Batch : 2500\t Loss : 2.2519686222076416\n",
      " Epoch : 4\t Batch : 3000\t Loss : 2.2347235679626465\n",
      " Epoch : 4\t Batch : 3500\t Loss : 2.2243552207946777\n",
      " Epoch : 4\t Batch : 4000\t Loss : 2.342594623565674\n",
      " Epoch : 4\t Batch : 4500\t Loss : 2.3166916370391846\n",
      " Epoch : 4\t Batch : 5000\t Loss : 2.2855329513549805\n",
      " Epoch : 4\t Batch : 5500\t Loss : 2.250732421875\n",
      " Epoch : 4\t Batch : 6000\t Loss : 2.3164279460906982\n",
      " Epoch : 4\t Batch : 6500\t Loss : 2.2748143672943115\n",
      " Epoch : 4\t Batch : 7000\t Loss : 2.24171781539917\n",
      " Epoch : 4\t Batch : 7500\t Loss : 2.413703441619873\n",
      " Epoch : 4\t Batch : 8000\t Loss : 2.229874849319458\n",
      " Epoch : 4\t Batch : 8500\t Loss : 2.2633252143859863\n",
      " Epoch : 4\t Batch : 9000\t Loss : 2.298074245452881\n",
      " Epoch : 4\t Batch : 9500\t Loss : 2.369354724884033\n",
      " Epoch : 4\t Batch : 10000\t Loss : 2.3456966876983643\n",
      " Epoch : 4\t Batch : 10500\t Loss : 2.28434681892395\n",
      " Epoch : 4\t Batch : 11000\t Loss : 2.3033363819122314\n",
      " Epoch : 4\t Batch : 11500\t Loss : 2.3581175804138184\n",
      " Epoch : 4\t Batch : 12000\t Loss : 2.228580951690674\n",
      " Epoch : 5\t Batch : 500\t Loss : 2.3018765449523926\n",
      " Epoch : 5\t Batch : 1000\t Loss : 2.2751219272613525\n",
      " Epoch : 5\t Batch : 1500\t Loss : 2.277334690093994\n",
      " Epoch : 5\t Batch : 2000\t Loss : 2.2824909687042236\n",
      " Epoch : 5\t Batch : 2500\t Loss : 2.281519651412964\n",
      " Epoch : 5\t Batch : 3000\t Loss : 2.3539462089538574\n",
      " Epoch : 5\t Batch : 3500\t Loss : 2.2593493461608887\n",
      " Epoch : 5\t Batch : 4000\t Loss : 2.284780502319336\n",
      " Epoch : 5\t Batch : 4500\t Loss : 2.3306784629821777\n",
      " Epoch : 5\t Batch : 5000\t Loss : 2.3556160926818848\n",
      " Epoch : 5\t Batch : 5500\t Loss : 2.3171818256378174\n",
      " Epoch : 5\t Batch : 6000\t Loss : 2.2477526664733887\n",
      " Epoch : 5\t Batch : 6500\t Loss : 2.2854080200195312\n",
      " Epoch : 5\t Batch : 7000\t Loss : 2.2693676948547363\n",
      " Epoch : 5\t Batch : 7500\t Loss : 2.285294771194458\n",
      " Epoch : 5\t Batch : 8000\t Loss : 2.3329086303710938\n",
      " Epoch : 5\t Batch : 8500\t Loss : 2.2837166786193848\n",
      " Epoch : 5\t Batch : 9000\t Loss : 2.299940824508667\n",
      " Epoch : 5\t Batch : 9500\t Loss : 2.34440279006958\n",
      " Epoch : 5\t Batch : 10000\t Loss : 2.3039603233337402\n",
      " Epoch : 5\t Batch : 10500\t Loss : 2.33980131149292\n",
      " Epoch : 5\t Batch : 11000\t Loss : 2.3227880001068115\n",
      " Epoch : 5\t Batch : 11500\t Loss : 2.3241665363311768\n",
      " Epoch : 5\t Batch : 12000\t Loss : 2.330848217010498\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 6\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Train\n",
    "    for batch, (X_train, y_train) in enumerate(train_loader):\n",
    "        batch += 1 # add 1 to batch to start count from 1, initially is be 0\n",
    "        y_pred = model(X_train) # train model\n",
    "        loss = loss_function(y_pred, y_train)\n",
    "\n",
    "        # add train loss to train_losses\n",
    "        train_losses.append(loss)\n",
    "\n",
    "        #print training state parameters in every 500 batch\n",
    "        if batch % 500 == 0:\n",
    "            print(f\" Epoch : {epoch}\\t Batch : {batch}\\t Loss : {loss.item()}\")\n",
    "        #optimize model parameter\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "    # Test\n",
    "    with torch.no_grad():\n",
    "        for batch, (X_test, y_test) in enumerate(test_loader):\n",
    "            y_eval = model(X_test) # test model\n",
    "            loss = loss_function(y_eval, y_test) # add loss function\n",
    "\n",
    "            \n",
    "            # add train loss to train_losses\n",
    "            test_losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_losses \u001b[38;5;241m=\u001b[39m [i\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_losses\u001b[49m]\n\u001b[0;32m      2\u001b[0m test_losses \u001b[38;5;241m=\u001b[39m [i\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m test_losses]\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(test_losses)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_losses' is not defined"
     ]
    }
   ],
   "source": [
    "train_losses = [i.item() for i in train_losses]\n",
    "test_losses = [i.item() for i in test_losses]\n",
    "plt.plot(test_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2472662925720215"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_losses[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
